#!/usr/bin/env python3
"""
è®°å¿†æ£€ç´¢æ¨¡å— - é«˜æ•ˆæ£€ç´¢ä¸æ¨ç†é“¾å¤ç”¨

æœ¬æ¨¡å—å®ç°è®°å¿†çš„é«˜æ•ˆæ£€ç´¢åŠŸèƒ½ï¼š
1. æ¨¡ç³ŠåŒ¹é…æ£€ç´¢ï¼ˆåŸºäº fuzzywuzzyï¼‰
2. å¤šç­–ç•¥åŒ¹é…ï¼ˆç²¾ç¡®/æ¨¡ç³Š/å…³é”®è¯ï¼‰
3. æ¨ç†é“¾æ™ºèƒ½å¤ç”¨
4. æ£€ç´¢æ€§èƒ½ä¼˜åŒ–

ä½œè€…ï¼šGUI Agent Team
æ—¥æœŸï¼š2024-12
"""

import json
import time
from datetime import datetime
from typing import Optional, Dict, List, Tuple, Any

# å¯¼å…¥è®°å¿†å­˜å‚¨æ¨¡å—
from memory_store import (
    list_trajectories,
    search_trajectories,
    extract_keywords,
    STORAGE_DIR
)

# å°è¯•å¯¼å…¥ fuzzywuzzyï¼ˆæ¨¡ç³ŠåŒ¹é…åº“ï¼‰
try:
    from fuzzywuzzy import fuzz
    HAS_FUZZYWUZZY = True
except ImportError:
    HAS_FUZZYWUZZY = False
    print("æç¤º: fuzzywuzzy æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€åŒ¹é…ã€‚")
    print("å®‰è£…å‘½ä»¤: pip install fuzzywuzzy python-Levenshtein")

# å°è¯•å¯¼å…¥éº’éºŸAIæ¡†æ¶ï¼ˆè¯­ä¹‰æ£€ç´¢ï¼‰
HAS_KYLIN_AI = False
KYLIN_EMBED_MODEL = None
try:
    # æ£€æŸ¥æ˜¯å¦æœ‰éº’éºŸAIæ¡†æ¶çš„embeddingæ¥å£
    import subprocess
    result = subprocess.run(["which", "kylin-llm-embed"], capture_output=True, timeout=2)
    if result.returncode == 0:
        HAS_KYLIN_AI = True
        print("âœ“ æ£€æµ‹åˆ°éº’éºŸAIæ¡†æ¶ï¼Œå°†å¯ç”¨è¯­ä¹‰æ£€ç´¢")
except:
    pass

# å¦‚æœæ²¡æœ‰éº’éºŸAIæ¡†æ¶ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–embeddingåº“ï¼ˆå¦‚sentence-transformersï¼‰
if not HAS_KYLIN_AI:
    try:
        import sentence_transformers
        HAS_SENTENCE_TRANSFORMERS = True
        # ä½¿ç”¨è½»é‡çº§ä¸­æ–‡æ¨¡å‹
        try:
            KYLIN_EMBED_MODEL = sentence_transformers.SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ“ ä½¿ç”¨ sentence-transformers è¿›è¡Œè¯­ä¹‰æ£€ç´¢")
        except:
            HAS_SENTENCE_TRANSFORMERS = False
    except ImportError:
        HAS_SENTENCE_TRANSFORMERS = False
        print("æç¤º: æœªæ£€æµ‹åˆ°è¯­ä¹‰æ£€ç´¢åº“ï¼Œå°†ä½¿ç”¨å…³é”®è¯æ£€ç´¢")


# ============================================================
# ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°
# ============================================================

def calculate_text_similarity(text1: str, text2: str, method: str = "token_sort") -> int:
    """
    è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦
    
    Args:
        text1: æ–‡æœ¬1
        text2: æ–‡æœ¬2
        method: åŒ¹é…æ–¹æ³• (token_sort/partial/simple)
        
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if not text1 or not text2:
        return 0
    
    text1 = text1.lower().strip()
    text2 = text2.lower().strip()
    
    if HAS_FUZZYWUZZY:
        if method == "token_sort":
            # åˆ†è¯æ’åºååŒ¹é…ï¼ˆå¯¹è¯åºä¸æ•æ„Ÿï¼‰
            return fuzz.token_sort_ratio(text1, text2)
        elif method == "partial":
            # éƒ¨åˆ†åŒ¹é…ï¼ˆé€‚åˆé•¿æ–‡æœ¬åŒ…å«çŸ­æ–‡æœ¬çš„æƒ…å†µï¼‰
            return fuzz.partial_ratio(text1, text2)
        elif method == "token_set":
            # é›†åˆåŒ¹é…ï¼ˆå»é‡åæ¯”è¾ƒï¼‰
            return fuzz.token_set_ratio(text1, text2)
        else:
            # ç®€å•åŒ¹é…
            return fuzz.ratio(text1, text2)
    else:
        # åŸºç¡€åŒ¹é…ï¼šåŸºäºå…³é”®è¯é‡å 
        words1 = set(text1.split())
        words2 = set(text2.split())
        if not words1 or not words2:
            return 0
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        return int(intersection / union * 100) if union > 0 else 0


def calculate_keyword_similarity(keywords1: List[str], keywords2: List[str]) -> int:
    """
    è®¡ç®—å…³é”®è¯åˆ—è¡¨çš„ç›¸ä¼¼åº¦ï¼ˆJaccard ç›¸ä¼¼åº¦ï¼‰
    
    Args:
        keywords1: å…³é”®è¯åˆ—è¡¨1
        keywords2: å…³é”®è¯åˆ—è¡¨2
        
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if not keywords1 or not keywords2:
        return 0
    
    set1 = set(k.lower() for k in keywords1)
    set2 = set(k.lower() for k in keywords2)
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return int(intersection / union * 100) if union > 0 else 0


def calculate_combined_similarity(
    task1: str,
    task2: str,
    keywords1: List[str],
    keywords2: List[str],
    weights: Tuple[float, float, float] = (0.5, 0.3, 0.2)
) -> int:
    """
    è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦ï¼ˆç»“åˆå¤šç§åŒ¹é…ç­–ç•¥ï¼‰
    
    Args:
        task1: ä»»åŠ¡æè¿°1
        task2: ä»»åŠ¡æè¿°2
        keywords1: å…³é”®è¯åˆ—è¡¨1
        keywords2: å…³é”®è¯åˆ—è¡¨2
        weights: (æ–‡æœ¬ç›¸ä¼¼åº¦, éƒ¨åˆ†åŒ¹é…, å…³é”®è¯ç›¸ä¼¼åº¦) æƒé‡
        
    Returns:
        ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    w1, w2, w3 = weights
    
    # æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆåˆ†è¯æ’åºï¼‰
    text_sim = calculate_text_similarity(task1, task2, "token_sort")
    
    # éƒ¨åˆ†åŒ¹é…ç›¸ä¼¼åº¦
    partial_sim = calculate_text_similarity(task1, task2, "partial")
    
    # å…³é”®è¯ç›¸ä¼¼åº¦
    keyword_sim = calculate_keyword_similarity(keywords1, keywords2)
    
    # åŠ æƒç»¼åˆ
    combined = int(w1 * text_sim + w2 * partial_sim + w3 * keyword_sim)
    
    return min(combined, 100)


# ============================================================
# æ ¸å¿ƒæ£€ç´¢å‡½æ•°
# ============================================================

def semantic_retrieve(
    user_task: str,
    threshold: float = 0.6,
    limit: int = 50,
    success_only: bool = True,
    verbose: bool = True
) -> List[Tuple[Dict, float]]:
    """
    è¯­ä¹‰æ£€ç´¢ï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
    
    Args:
        user_task: ç”¨æˆ·ä»»åŠ¡æè¿°
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        limit: æœ€å¤šæ£€ç´¢çš„è½¨è¿¹æ•°é‡
        success_only: æ˜¯å¦åªæ£€ç´¢æˆåŠŸçš„è½¨è¿¹
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        [(è½¨è¿¹, ç›¸ä¼¼åº¦), ...] åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åº
    """
    if not HAS_KYLIN_AI and not HAS_SENTENCE_TRANSFORMERS:
        if verbose:
            print("âš ï¸ è¯­ä¹‰æ£€ç´¢ä¸å¯ç”¨ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
        # å›é€€åˆ°å…³é”®è¯æ£€ç´¢
        return [(t, s/100.0) for t, s in retrieve_top_k_trajectories(user_task, k=3, limit=limit, success_only=success_only)]
    
    trajectories = list_trajectories(limit=limit, success_only=success_only)
    if not trajectories:
        return []
    
    # å°†ç”¨æˆ·ä»»åŠ¡è½¬æ¢ä¸ºå‘é‡
    try:
        if HAS_KYLIN_AI:
            # ä½¿ç”¨éº’éºŸAIæ¡†æ¶
            import subprocess
            result = subprocess.run(
                ["kylin-llm-embed", "--text", user_task],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                query_vector = json.loads(result.stdout)
            else:
                return []
        else:
            # ä½¿ç”¨sentence-transformers
            query_vector = KYLIN_EMBED_MODEL.encode(user_task).tolist()
    except Exception as e:
        if verbose:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
        return [(t, s/100.0) for t, s in retrieve_top_k_trajectories(user_task, k=3, limit=limit, success_only=success_only)]
    
    # è®¡ç®—ä¸å†å²è½¨è¿¹çš„ç›¸ä¼¼åº¦
    scored_trajectories = []
    for traj in trajectories:
        history_task = traj.get("task", "")
        if not history_task:
            continue
        
        try:
            if HAS_KYLIN_AI:
                result = subprocess.run(
                    ["kylin-llm-embed", "--text", history_task],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    history_vector = json.loads(result.stdout)
                else:
                    continue
            else:
                history_vector = KYLIN_EMBED_MODEL.encode(history_task).tolist()
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            import numpy as np
            query_np = np.array(query_vector)
            history_np = np.array(history_vector)
            similarity = np.dot(query_np, history_np) / (np.linalg.norm(query_np) * np.linalg.norm(history_np))
            
            if similarity >= threshold:
                scored_trajectories.append((traj, float(similarity)))
        except Exception as e:
            if verbose:
                print(f"âš ï¸ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            continue
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    scored_trajectories.sort(key=lambda x: x[1], reverse=True)
    
    if verbose:
        print(f"è¯­ä¹‰æ£€ç´¢æ‰¾åˆ° {len(scored_trajectories)} æ¡ç›¸ä¼¼è½¨è¿¹ï¼ˆé˜ˆå€¼â‰¥{threshold:.0%}ï¼‰")
    
    return scored_trajectories[:3]  # è¿”å›å‰3ä¸ª


def retrieve_similar_trajectory(
    user_task: str,
    threshold: int = 70,
    limit: int = 50,
    success_only: bool = True,
    verbose: bool = True,
    use_semantic: bool = False
) -> Optional[Dict]:
    """
    æ£€ç´¢æœ€ç›¸ä¼¼çš„å†å²è½¨è¿¹
    
    Args:
        user_task: ç”¨æˆ·å½“å‰ä»»åŠ¡
        threshold: åŒ¹é…é˜ˆå€¼ (0-100)
        limit: æœ€å¤šæ£€ç´¢çš„è½¨è¿¹æ•°é‡
        success_only: æ˜¯å¦åªæ£€ç´¢æˆåŠŸçš„è½¨è¿¹
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        æœ€ç›¸ä¼¼çš„è½¨è¿¹å­—å…¸ï¼Œæ— åŒ¹é…è¿”å› None
    """
    start_time = time.time()
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"æ£€ç´¢ç›¸ä¼¼è½¨è¿¹")
        print(f"{'='*60}")
        print(f"å½“å‰ä»»åŠ¡: {user_task}")
        print(f"åŒ¹é…é˜ˆå€¼: {threshold}")
        print(f"æ£€ç´¢æ¨¡å¼: {'è¯­ä¹‰æ£€ç´¢' if use_semantic else 'å…³é”®è¯æ£€ç´¢'}")
    
    # å¦‚æœå¯ç”¨è¯­ä¹‰æ£€ç´¢ä¸”å¯ç”¨
    if use_semantic and (HAS_KYLIN_AI or HAS_SENTENCE_TRANSFORMERS):
        semantic_results = semantic_retrieve(
            user_task=user_task,
            threshold=threshold/100.0,
            limit=limit,
            success_only=success_only,
            verbose=verbose
        )
        if semantic_results:
            best_match, best_score = semantic_results[0]
            if verbose:
                print(f"\nâœ“ è¯­ä¹‰æ£€ç´¢æ‰¾åˆ°åŒ¹é…è½¨è¿¹ï¼ˆç›¸ä¼¼åº¦: {best_score:.2%}ï¼‰")
            return best_match
    
    # è·å–å†å²è½¨è¿¹ï¼ˆå…³é”®è¯æ£€ç´¢ï¼‰
    trajectories = list_trajectories(limit=limit, success_only=success_only)
    
    if not trajectories:
        if verbose:
            print("âš ï¸ æ— å†å²åä½œè½¨è¿¹")
        return None
    
    if verbose:
        print(f"æ£€ç´¢èŒƒå›´: æœ€è¿‘ {len(trajectories)} æ¡è½¨è¿¹")
    
    # æå–å½“å‰ä»»åŠ¡çš„å…³é”®è¯
    current_keywords = extract_keywords(user_task)
    
    # éå†è®¡ç®—ç›¸ä¼¼åº¦
    best_match = None
    best_score = 0
    match_details = []
    
    for traj in trajectories:
        history_task = traj.get("task", "")
        history_keywords = traj.get("keywords", [])
        
        if not history_task:
            continue
        
        # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
        score = calculate_combined_similarity(
            user_task, history_task,
            current_keywords, history_keywords
        )
        
        match_details.append({
            "task": history_task[:40] + "..." if len(history_task) > 40 else history_task,
            "score": score
        })
        
        if score > best_score:
            best_score = score
            best_match = traj
    
    elapsed = time.time() - start_time
    
    # æ‰“å°åŒ¹é…è¯¦æƒ…
    if verbose:
        print(f"\n--- åŒ¹é…ç»“æœï¼ˆè€—æ—¶: {elapsed:.3f}sï¼‰---")
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œæ˜¾ç¤ºå‰5ä¸ª
        sorted_matches = sorted(match_details, key=lambda x: x["score"], reverse=True)[:5]
        for m in sorted_matches:
            indicator = "âœ“" if m["score"] >= threshold else " "
            print(f"  {indicator} [{m['score']:3d}] {m['task']}")
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
    if best_match and best_score >= threshold:
        if verbose:
            print(f"\nâœ“ æ‰¾åˆ°åŒ¹é…è½¨è¿¹ï¼ˆç›¸ä¼¼åº¦: {best_score}ï¼‰")
        return best_match
    else:
        if verbose:
            print(f"\nâš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ â‰¥ {threshold} çš„è½¨è¿¹")
        return None


def retrieve_top_k_trajectories(
    user_task: str,
    k: int = 5,
    limit: int = 50,
    success_only: bool = True
) -> List[Tuple[Dict, int]]:
    """
    æ£€ç´¢ Top-K ç›¸ä¼¼è½¨è¿¹
    
    Args:
        user_task: ç”¨æˆ·å½“å‰ä»»åŠ¡
        k: è¿”å›æ•°é‡
        limit: æœ€å¤šæ£€ç´¢çš„è½¨è¿¹æ•°é‡
        success_only: æ˜¯å¦åªæ£€ç´¢æˆåŠŸçš„è½¨è¿¹
        
    Returns:
        [(è½¨è¿¹, ç›¸ä¼¼åº¦), ...] åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åº
    """
    trajectories = list_trajectories(limit=limit, success_only=success_only)
    
    if not trajectories:
        return []
    
    current_keywords = extract_keywords(user_task)
    
    scored_trajectories = []
    for traj in trajectories:
        history_task = traj.get("task", "")
        history_keywords = traj.get("keywords", [])
        
        if not history_task:
            continue
        
        score = calculate_combined_similarity(
            user_task, history_task,
            current_keywords, history_keywords
        )
        scored_trajectories.append((traj, score))
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    scored_trajectories.sort(key=lambda x: x[1], reverse=True)
    
    return scored_trajectories[:k]


# ============================================================
# æ¨ç†é“¾å¤ç”¨å‡½æ•°
# ============================================================

def reuse_reasoning_chain(
    user_task: str,
    threshold: int = 70,
    verbose: bool = True
) -> Optional[Dict]:
    """
    å¤ç”¨ç›¸ä¼¼è½¨è¿¹çš„æ¨ç†é“¾
    
    Args:
        user_task: ç”¨æˆ·å½“å‰ä»»åŠ¡
        threshold: åŒ¹é…é˜ˆå€¼
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        å¤ç”¨çš„æ¨ç†é“¾å­—å…¸ï¼Œæ— åŒ¹é…è¿”å› None
    """
    similar_traj = retrieve_similar_trajectory(
        user_task=user_task,
        threshold=threshold,
        verbose=verbose
    )
    
    if not similar_traj:
        return None
    
    # æå–å†å²æ¨ç†é“¾
    history_reasoning = similar_traj.get("reasoning_chain", {})
    
    if not history_reasoning:
        if verbose:
            print("âš ï¸ åŒ¹é…çš„è½¨è¿¹æ— æœ‰æ•ˆæ¨ç†é“¾")
        return None
    
    # æ·»åŠ å¤ç”¨æ ‡è®°
    reused_reasoning = history_reasoning.copy()
    reused_reasoning["_reused"] = True
    reused_reasoning["_reused_from"] = {
        "task": similar_traj.get("task"),
        "task_hash": similar_traj.get("task_hash"),
        "timestamp": similar_traj.get("timestamp"),
        "success": similar_traj.get("success")
    }
    
    if verbose:
        print(f"\n--- å¤ç”¨çš„æ¨ç†é“¾ ---")
        tc = history_reasoning.get("thought_chain", {})
        print(f"ä»»åŠ¡åˆ†è§£: {tc.get('task_decomposition', 'N/A')[:80]}...")
    
    return reused_reasoning


# ============================================================
# æ£€ç´¢ä¼˜å…ˆçš„æ¨ç†æµç¨‹
# ============================================================

def reasoning_with_retrieval(
    user_task: str,
    threshold: int = 70,
    verbose: bool = True
) -> Tuple[Dict, str]:
    """
    æ£€ç´¢ä¼˜å…ˆï¼šå…ˆæŸ¥å†å²è½¨è¿¹ï¼Œæ— åŒ¹é…å†ç”Ÿæˆæ–°æ¨ç†é“¾
    
    Args:
        user_task: ç”¨æˆ·ä»»åŠ¡
        threshold: åŒ¹é…é˜ˆå€¼
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        (æ¨ç†é“¾, çŠ¶æ€) çŠ¶æ€ä¸º "reused" æˆ– "generated"
    """
    # 1. å°è¯•æ£€ç´¢å¤ç”¨
    reused_reasoning = reuse_reasoning_chain(
        user_task=user_task,
        threshold=threshold,
        verbose=verbose
    )
    
    if reused_reasoning:
        return reused_reasoning, "reused"
    
    # 2. æ— åŒ¹é…ï¼Œè°ƒç”¨ system2_memory ç”Ÿæˆæ–°æ¨ç†é“¾
    if verbose:
        print("\n--- ç”Ÿæˆæ–°æ¨ç†é“¾ ---")
    
    try:
        from system2_memory import reasoning_with_memory
        
        reasoning_chain, _ = reasoning_with_memory(
            user_task=user_task,
            enable_reuse=False,  # å·²ç»æ£€ç´¢è¿‡äº†ï¼Œä¸éœ€è¦å†æ£€ç´¢
            verbose=verbose
        )
        return reasoning_chain, "generated"
    except Exception as e:
        if verbose:
            print(f"âš ï¸ ç”Ÿæˆæ¨ç†é“¾å¤±è´¥: {e}")
        # è¿”å›åŸºç¡€å…œåº•æ¨ç†é“¾
        return {
            "thought_chain": {
                "task_decomposition": f"æ‰§è¡Œ: {user_task}",
                "agent_selection": [{"step": 1, "agent": "DefaultAgent", "reason": "é»˜è®¤å¤„ç†"}],
                "risk_assessment": "æœªçŸ¥é£é™©",
                "fallback_plan": "æ‰‹åŠ¨å¹²é¢„"
            },
            "execution_plan": [{"step": 1, "action": user_task, "agent": "DefaultAgent"}],
            "milestone_markers": ["start", "execute", "complete"],
            "_is_fallback": True
        }, "fallback"


# ============================================================
# æ£€ç´¢ç»Ÿè®¡
# ============================================================

def get_retrieval_stats() -> Dict:
    """
    è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    trajectories = list_trajectories(limit=1000)
    
    stats = {
        "total_trajectories": len(trajectories),
        "success_trajectories": sum(1 for t in trajectories if t.get("success")),
        "agents_distribution": {},
        "keywords_frequency": {},
        "recent_tasks": []
    }
    
    for traj in trajectories:
        # æ™ºèƒ½ä½“åˆ†å¸ƒ
        for agent in traj.get("agents_involved", []):
            stats["agents_distribution"][agent] = stats["agents_distribution"].get(agent, 0) + 1
        
        # å…³é”®è¯é¢‘ç‡
        for kw in traj.get("keywords", []):
            stats["keywords_frequency"][kw] = stats["keywords_frequency"].get(kw, 0) + 1
    
    # æœ€è¿‘ä»»åŠ¡
    stats["recent_tasks"] = [
        {"task": t.get("task", "")[:50], "timestamp": t.get("timestamp")}
        for t in trajectories[:5]
    ]
    
    # å…³é”®è¯æŒ‰é¢‘ç‡æ’åº
    stats["top_keywords"] = sorted(
        stats["keywords_frequency"].items(),
        key=lambda x: x[1],
        reverse=True
    )[:10]
    
    return stats


# ============================================================
# æµ‹è¯•å‡½æ•°
# ============================================================

def test_retrieval():
    """æµ‹è¯•è®°å¿†æ£€ç´¢åŠŸèƒ½"""
    print("\n" + "ğŸ” è®°å¿†æ£€ç´¢æ¨¡å—æµ‹è¯• ğŸ”".center(60))
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å†å²è½¨è¿¹
    trajectories = list_trajectories(limit=10)
    if not trajectories:
        print("âš ï¸ æ— å†å²è½¨è¿¹ï¼Œè¯·å…ˆè¿è¡Œ system2_memory.py ç”Ÿæˆæµ‹è¯•æ•°æ®")
        return
    
    print(f"âœ“ æ£€æµ‹åˆ° {len(trajectories)} æ¡å†å²è½¨è¿¹")
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "å®Œå…¨åŒ¹é…",
            "task": trajectories[0].get("task", "æµ‹è¯•ä»»åŠ¡"),
            "expected": "high"
        },
        {
            "name": "ç›¸ä¼¼ä»»åŠ¡",
            "task": "æœç´¢ä¸‹è½½ç›®å½•çš„jpgæ–‡ä»¶è®¾ç½®ä¸ºå£çº¸",
            "expected": "medium"
        },
        {
            "name": "ä¸åŒ¹é…ä»»åŠ¡",
            "task": "æŸ¥çœ‹ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µå¹¶ç”ŸæˆæŠ¥å‘Š",
            "expected": "low"
        }
    ]
    
    results = []
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n\n{'#' * 60}")
        print(f"# æµ‹è¯• {i}: {case['name']}")
        print(f"# ä»»åŠ¡: {case['task'][:50]}...")
        print(f"{'#' * 60}")
        
        # æ£€ç´¢
        reasoning, status = reasoning_with_retrieval(
            user_task=case["task"],
            threshold=60,
            verbose=True
        )
        
        results.append({
            "name": case["name"],
            "task": case["task"],
            "status": status,
            "reused": status == "reused"
        })
    
    # æ‰“å°æµ‹è¯•æ€»ç»“
    print("\n\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    for r in results:
        status_icon = "âœ“" if r["reused"] else "â—‹"
        print(f"  {status_icon} {r['name']}: {r['status']}")
    
    # æ£€ç´¢ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æ£€ç´¢ç»Ÿè®¡")
    print("=" * 60)
    
    stats = get_retrieval_stats()
    print(f"  æ€»è½¨è¿¹æ•°: {stats['total_trajectories']}")
    print(f"  æˆåŠŸè½¨è¿¹: {stats['success_trajectories']}")
    print(f"  æ™ºèƒ½ä½“åˆ†å¸ƒ: {stats['agents_distribution']}")
    print(f"  çƒ­é—¨å…³é”®è¯: {stats['top_keywords'][:5]}")
    
    print("\n" + "=" * 60)
    print("âœ“ æ£€ç´¢æµ‹è¯•å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    test_retrieval()

